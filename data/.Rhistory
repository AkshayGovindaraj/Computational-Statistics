summary(model_simple)
#Plotting the data and line
plot(df$x, df$y)
abline(model_simple)
#Implementing a weighted linear model
model_weight <- lm(y~x, data=df, weights=exp((-df$x^2)/20))
summary(model_weight)
#Plotting the data and line
plot(df$x, df$y)
abline(model_weight)
### Solving simple least squares using gradient descent
#Cost Function
cost <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y) ) #*(exp(X*X/20)))
}
cost_weight <- function(X, y, theta) {
sum( t(X %*% theta - y) %*% w %*% (X %*% theta - y) ) / (2*length(y) ) #*(exp(X*X/20)))
}
# learning rate and iteration limit
alpha <- 0.00005
num_iters <- 1000
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
# initialize coefficients
theta <- matrix(c(0,0), nrow=2)
# add a column of 1's for the intercept coefficient
X <- cbind(1, x_data)
X <- data.matrix(X, rownames.force = NA)
y <- data.matrix(y_data, rownames.force = NA)
#Weight matrix
w = X %*% t(X)
w = diag(w)
w = diag(w)
# gradient descent
for (i in 1:num_iters) {
error <- (X %*% theta - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost_weight(X, y, theta)
theta_history[[i]] <- theta
}
print(theta)
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
# Computational Stats Assignment 2 Question 2
# Logistic Regression
x_data <- read.delim("logit-x.dat", header = FALSE, sep="")
y_data <- read.delim("logit-y.dat", header = FALSE, sep="\t")
total <- merge(y_data,x_data, by=c("V1","V2"))
#Logistic Regression
model <- glm(y_data[1]$V1~x_data$V1 + x_data$V2,family=binomial(link='logit'))
summary(model)
#Manually implementing gradient descent
library(LaplacesDemon)
#Cost Function
cost_logit <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y) ) #*(exp(X*X/20)))
}
# learning rate and iteration limit
alpha <- 0.0001
num_iters <- 1000
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
# initialize coefficients
theta <- matrix(c(0,0,0), nrow=3)
# add a column of 1's for the intercept coefficient
X <- cbind(1, x_data)
X <- data.matrix(X, rownames.force = NA)
y <- data.matrix(y_data, rownames.force = NA)
# gradient descent
for (i in 1:num_iters) {
error <- (invlogit(X %*% theta) - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost_logit(X, y, theta)
theta_history[[i]] <- theta
}
print(theta)
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
# Computational Stats Assignment 2 Question 2
# Logistic Regression
x_data <- read.delim("logit-x.dat", header = FALSE, sep="")
y_data <- read.delim("logit-y.dat", header = FALSE, sep="\t")
total <- merge(y_data,x_data, by=c("V1","V2"))
#Logistic Regression
model <- glm(y_data[1]$V1~x_data$V1 + x_data$V2,family=binomial(link='logit'))
summary(model)
#Manually implementing gradient descent
library(LaplacesDemon)
#Cost Function
cost_logit <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y) ) #*(exp(X*X/20)))
}
# learning rate and iteration limit
alpha <- 0.001
num_iters <- 1000
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
# initialize coefficients
theta <- matrix(c(0,0,0), nrow=3)
# add a column of 1's for the intercept coefficient
X <- cbind(1, x_data)
X <- data.matrix(X, rownames.force = NA)
y <- data.matrix(y_data, rownames.force = NA)
# gradient descent
for (i in 1:num_iters) {
error <- (invlogit(X %*% theta) - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost_logit(X, y, theta)
theta_history[[i]] <- theta
}
print(theta)
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
# Computational Stats Assignment 2 Question 2
# Logistic Regression
x_data <- read.delim("logit-x.dat", header = FALSE, sep="")
y_data <- read.delim("logit-y.dat", header = FALSE, sep="\t")
total <- merge(y_data,x_data, by=c("V1","V2"))
#Logistic Regression
model <- glm(y_data[1]$V1~x_data$V1 + x_data$V2,family=binomial(link='logit'))
summary(model)
#Manually implementing gradient descent
library(LaplacesDemon)
#Cost Function
cost_logit <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y) ) #*(exp(X*X/20)))
}
# learning rate and iteration limit
alpha <- 0.0005
num_iters <- 1000
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
# initialize coefficients
theta <- matrix(c(0,0,0), nrow=3)
# add a column of 1's for the intercept coefficient
X <- cbind(1, x_data)
X <- data.matrix(X, rownames.force = NA)
y <- data.matrix(y_data, rownames.force = NA)
# gradient descent
for (i in 1:num_iters) {
error <- (invlogit(X %*% theta) - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost_logit(X, y, theta)
theta_history[[i]] <- theta
}
print(theta)
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
# Computational Stats Assignment 2 Question 2
# Logistic Regression
x_data <- read.delim("logit-x.dat", header = FALSE, sep="")
y_data <- read.delim("logit-y.dat", header = FALSE, sep="\t")
total <- merge(y_data,x_data, by=c("V1","V2"))
#Logistic Regression
model <- glm(y_data[1]$V1~x_data$V1 + x_data$V2,family=binomial(link='logit'))
summary(model)
#Manually implementing gradient descent
library(LaplacesDemon)
#Cost Function
cost_logit <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y) ) #*(exp(X*X/20)))
}
# learning rate and iteration limit
alpha <- 0.0002
num_iters <- 1000
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
# initialize coefficients
theta <- matrix(c(0,0,0), nrow=3)
# add a column of 1's for the intercept coefficient
X <- cbind(1, x_data)
X <- data.matrix(X, rownames.force = NA)
y <- data.matrix(y_data, rownames.force = NA)
# gradient descent
for (i in 1:num_iters) {
error <- (invlogit(X %*% theta) - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost_logit(X, y, theta)
theta_history[[i]] <- theta
}
print(theta)
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
# Computational Stats Assignment 2 Question 2
# Logistic Regression
x_data <- read.delim("logit-x.dat", header = FALSE, sep="")
y_data <- read.delim("logit-y.dat", header = FALSE, sep="\t")
total <- merge(y_data,x_data, by=c("V1","V2"))
#Logistic Regression
model <- glm(y_data[1]$V1~x_data$V1 + x_data$V2,family=binomial(link='logit'))
summary(model)
#Manually implementing gradient descent
library(LaplacesDemon)
#Cost Function
cost_logit <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y) ) #*(exp(X*X/20)))
}
# learning rate and iteration limit
alpha <- 0.0002
num_iters <- 10000
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
# initialize coefficients
theta <- matrix(c(0,0,0), nrow=3)
# add a column of 1's for the intercept coefficient
X <- cbind(1, x_data)
X <- data.matrix(X, rownames.force = NA)
y <- data.matrix(y_data, rownames.force = NA)
# gradient descent
for (i in 1:num_iters) {
error <- (invlogit(X %*% theta) - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost_logit(X, y, theta)
theta_history[[i]] <- theta
}
print(theta)
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
# Computational Stats Assignment 2 Question 2
# Logistic Regression
x_data <- read.delim("logit-x.dat", header = FALSE, sep="")
y_data <- read.delim("logit-y.dat", header = FALSE, sep="\t")
total <- merge(y_data,x_data, by=c("V1","V2"))
#Logistic Regression
model <- glm(y_data[1]$V1~x_data$V1 + x_data$V2,family=binomial(link='logit'))
summary(model)
#Manually implementing gradient descent
library(LaplacesDemon)
#Cost Function
cost_logit <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y) ) #*(exp(X*X/20)))
}
# learning rate and iteration limit
alpha <- 0.0001
num_iters <- 10000
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
# initialize coefficients
theta <- matrix(c(0,0,0), nrow=3)
# add a column of 1's for the intercept coefficient
X <- cbind(1, x_data)
X <- data.matrix(X, rownames.force = NA)
y <- data.matrix(y_data, rownames.force = NA)
# gradient descent
for (i in 1:num_iters) {
error <- (invlogit(X %*% theta) - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost_logit(X, y, theta)
theta_history[[i]] <- theta
}
print(theta)
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
# Computational Stats Assignment 2 Question 2
# Logistic Regression
x_data <- read.delim("logit-x.dat", header = FALSE, sep="")
y_data <- read.delim("logit-y.dat", header = FALSE, sep="\t")
total <- merge(y_data,x_data, by=c("V1","V2"))
#Manually implementing gradient descent
library(LaplacesDemon)
#Cost Function
cost_logit <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y) ) #*(exp(X*X/20)))
}
# learning rate and iteration limit
alpha <- 0.00001
num_iters <- 10000
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
# initialize coefficients
theta <- matrix(c(0,0,0), nrow=3)
# add a column of 1's for the intercept coefficient
X <- cbind(1, x_data)
X <- data.matrix(X, rownames.force = NA)
y <- data.matrix(y_data, rownames.force = NA)
# gradient descent
for (i in 1:num_iters) {
error <- (invlogit(X %*% theta) - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost_logit(X, y, theta)
theta_history[[i]] <- theta
}
print(theta)
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
summary(model)
# Computational Stats Assignment 2 Question 2
# Logistic Regression
x_data <- read.delim("logit-x.dat", header = FALSE, sep="")
y_data <- read.delim("logit-y.dat", header = FALSE, sep="\t")
total <- merge(y_data,x_data, by=c("V1","V2"))
#Logistic Regression
model <- glm(y_data[1]$V1~x_data$V1 + x_data$V2,family=binomial(link='logit'))
summary(model)
#Manually implementing gradient descent
library(LaplacesDemon)
#Cost Function
cost_logit <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y) ) #*(exp(X*X/20)))
}
# learning rate and iteration limit
alpha <- 0.0001
num_iters <- 100000
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
# initialize coefficients
theta <- matrix(c(0,0,0), nrow=3)
# add a column of 1's for the intercept coefficient
X <- cbind(1, x_data)
X <- data.matrix(X, rownames.force = NA)
y <- data.matrix(y_data, rownames.force = NA)
# gradient descent
for (i in 1:num_iters) {
error <- (invlogit(X %*% theta) - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost_logit(X, y, theta)
theta_history[[i]] <- theta
}
print(theta)
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
# Computational Stats Assignment 2 Question 2
# Logistic Regression
x_data <- read.delim("logit-x.dat", header = FALSE, sep="")
y_data <- read.delim("logit-y.dat", header = FALSE, sep="\t")
total <- merge(y_data,x_data, by=c("V1","V2"))
#Logistic Regression
model <- glm(y_data[1]$V1~x_data$V1 + x_data$V2,family=binomial(link='logit'))
summary(model)
#Manually implementing gradient descent
library(LaplacesDemon)
#Cost Function
cost_logit <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y) ) #*(exp(X*X/20)))
}
# learning rate and iteration limit
alpha <- 0.00001
num_iters <- 100000
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
# initialize coefficients
theta <- matrix(c(0,0,0), nrow=3)
# add a column of 1's for the intercept coefficient
X <- cbind(1, x_data)
X <- data.matrix(X, rownames.force = NA)
y <- data.matrix(y_data, rownames.force = NA)
# gradient descent
for (i in 1:num_iters) {
error <- (invlogit(X %*% theta) - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost_logit(X, y, theta)
theta_history[[i]] <- theta
}
print(theta)
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
# Computational Stats Assignment 2 Question 2
# Logistic Regression
x_data <- read.delim("logit-x.dat", header = FALSE, sep="")
y_data <- read.delim("logit-y.dat", header = FALSE, sep="\t")
total <- merge(y_data,x_data, by=c("V1","V2"))
#Logistic Regression
model <- glm(y_data[1]$V1~x_data$V1 + x_data$V2,family=binomial(link='logit'))
summary(model)
#Manually implementing gradient descent
library(LaplacesDemon)
#Cost Function
cost_logit <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y) ) #*(exp(X*X/20)))
}
# learning rate and iteration limit
alpha <- 0.0001
num_iters <- 1000
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
# initialize coefficients
theta <- matrix(c(0,0,0), nrow=3)
# add a column of 1's for the intercept coefficient
X <- cbind(1, x_data)
X <- data.matrix(X, rownames.force = NA)
y <- data.matrix(y_data, rownames.force = NA)
# gradient descent
for (i in 1:num_iters) {
error <- (invlogit(X %*% theta) - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost_logit(X, y, theta)
theta_history[[i]] <- theta
}
print(theta)
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
# Computational Stats Assignment 2 Question 2
# Logistic Regression
x_data <- read.delim("logit-x.dat", header = FALSE, sep="")
y_data <- read.delim("logit-y.dat", header = FALSE, sep="\t")
total <- merge(y_data,x_data, by=c("V1","V2"))
#Logistic Regression
model <- glm(y_data[1]$V1~x_data$V1 + x_data$V2,family=binomial(link='logit'))
summary(model)
#Manually implementing gradient descent
library(LaplacesDemon)
#Cost Function
cost_logit <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y) ) #*(exp(X*X/20)))
}
# learning rate and iteration limit
alpha <- 0.01
num_iters <- 1000
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
# initialize coefficients
theta <- matrix(c(0,0,0), nrow=3)
# add a column of 1's for the intercept coefficient
X <- cbind(1, x_data)
X <- data.matrix(X, rownames.force = NA)
y <- data.matrix(y_data, rownames.force = NA)
# gradient descent
for (i in 1:num_iters) {
error <- (invlogit(X %*% theta) - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost_logit(X, y, theta)
theta_history[[i]] <- theta
}
print(theta)
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
# Computational Stats Assignment 2 Question 3
# Locally weighted linear regression
x_data <- read.delim("rx.dat", header = FALSE, sep="")
y_data <- read.delim("ry.dat", header = FALSE, sep="")
df = data.frame(y_data, x_data)
colnames(df) <- c("y", "x")
#Implementing a simple linear model
model_simple <- lm(y~x, data=df)
summary(model_simple)
#Plotting the data and line
plot(df$x, df$y)
abline(model_simple)
#Implementing a weighted linear model
model_weight <- lm(y~x, data=df, weights=exp((-df$x^2)/20))
summary(model_weight)
#Plotting the data and line
plot(df$x, df$y)
abline(model_weight)
### Solving simple least squares using gradient descent
#Cost Function
cost <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y) ) #*(exp(X*X/20)))
}
cost_weight <- function(X, y, theta) {
sum( t(X %*% theta - y) %*% w %*% (X %*% theta - y) ) / (2*length(y) ) #*(exp(X*X/20)))
}
# learning rate and iteration limit
alpha <- 0.00005
num_iters <- 1000
# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)
# initialize coefficients
theta <- matrix(c(0,0), nrow=2)
# add a column of 1's for the intercept coefficient
X <- cbind(1, x_data)
X <- data.matrix(X, rownames.force = NA)
y <- data.matrix(y_data, rownames.force = NA)
#Weight matrix
w = X %*% t(X)
w = diag(w)
w = diag(w)
# gradient descent
for (i in 1:num_iters) {
error <- (X %*% theta - y)
delta <- t(X) %*% error / length(y)
theta <- theta - alpha * delta
cost_history[i] <- cost_weight(X, y, theta)
theta_history[[i]] <- theta
}
print(theta)
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
#Plotting the data and line
plot(df$x, df$y)
abline(model_simple)
summary(model_weight)
abline(model_weight)
#Assignment 2 of Computational Statistics
houseprices = read.csv("RealEstate.csv")
#Fitting linear model to predict price
price_pred = lm(Price ~. , data = houseprices)
summary(price_pred)
#Short Sale case
ssale <- houseprices[ which(houseprices$Status == 'Short Sale'), ]
price_pred1 = lm(Price ~ Bedrooms + Bathrooms + Size + Price.SQ.Ft + Location + MLS , data = ssale)
summary(price_pred1)
#Foreclosure case
foreclosure <- houseprices[ which(houseprices$Status == 'Foreclosure'), ]
price_pred2 = lm(Price ~ Bedrooms + Bathrooms + Size + Price.SQ.Ft + Location + MLS , data = foreclosure)
summary(price_pred2)
#Regular Case
regular <- houseprices[ which(houseprices$Status == 'Regular'), ]
price_pred1 = lm(Price ~ Bedrooms + Bathrooms + Size + Price.SQ.Ft + Location + MLS , data = regular)
summary(price_pred1)
